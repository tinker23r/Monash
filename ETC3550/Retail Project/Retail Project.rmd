---
title: "Retail Forecasting Project"
subtitle: "ETC3550"
author: "Chelaka Paranahewa"
output: 
  pdf_document:
    fig_crop: false
---

```{r setup, echo = FALSE, include = FALSE, warning=FALSE, error=FALSE}
library(fpp3, warn.conflicts = FALSE)
library(knitr, warn.conflicts = FALSE)
library(progressr, warn.conflicts = FALSE)
library(readabs, warn.conflicts = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
set.seed(31455034)
myseries <- aus_retail %>%
    # Remove discontinued series
    filter(!(`Series ID` %in% c(
        "A3349561R", "A3349883F", "A3349499L", "A3349902A",
        "A3349588R", "A3349763L", "A3349372C", "A3349450X",
        "A3349679W", "A3349378T", "A3349767W", "A3349451A"
    ))) %>%
    # Select a series at random
    filter(`Series ID` == sample(`Series ID`, 1))
```

# Statistical features of Australian Retail

```{r Original Dataset Table, echo = FALSE}
knitr::kable(
    head(myseries),
    caption = "A few rows of the dataset, Other Retailing in South Australia"
)
```

```{r Original Dataset Graph, echo = FALSE}
myseries %>% autoplot(Turnover) +
    labs(
        title = "Australia Retail Turnover",
        subtitle = "Original"
    ) +
    ylab("Turnover [million $AUD]")
```

<!-- Explain trends like variance -->
Plotting the original dataset, we can see a general trend upwards. The dataset also has a seasonal pattern which during the early 1980 - 1990, was relatively small compared to later years where the spike grows to large proportions. 

```{r Seasonal Plot of Dataset, echo = FALSE}
myseries %>%
    gg_season(Turnover, labels = "both") +
    labs(
        y = "Turnover [million $AUD]",
        title = "Seasonal Plot: Australia Retail Turnover"
    )
```

From the seasonal plot, there is a clear increase in retail sales during the month December which suggests that there is strong seasonality with the data. 

# Transformations and Differencings


### Transformations

To prepare the dataset for ARIMA modelling, the data needs to be tranformed so that the variance across the dataset remains relatively constant. As mentioned before the original dataset does not have a constant seasonal variation, since the beginning has small peaks in the seasonal variation which gradually grow over the course of approximately two decades. 

To stabalise the seasonal variation, a Box-Cox transformation can be use. A Box-Cox transformation needs a value for $\lambda$ to transformed the date. A $\lambda = 0$ will perform a logrithmic tranformation to the data, whereas $\lambda \ne 0$ will perform a exponential transformation. 

```{r, echo=FALSE, eval=FALSE}
# Natural Log
myseries %>% autoplot(box_cox(Turnover, 0.0)) +
    labs(
        title = "Australia Retail Turnover",
        subtitle = "Box-Cox transformed (Natural Log)"
    ) +
    ylab("Turnover [million $AUD]")
```

```{r Transforming Dataset and Graph for ETS Models}
lambda <- myseries %>%
    features(Turnover, features = guerrero) %>%
    pull(lambda_guerrero)

myseries %>% autoplot(box_cox(Turnover, lambda)) +
    labs(
        title = "Australia Retail Turnover",
        subtitle = "Box-Cox transformed (Guerrero's method)"
    ) +
    ylab("Turnover [million $AUD]")
```

Using a $\lambda$ value close to 0 such as `r lambda` gives the best transformation that keeps the seasonal variations constant through out the time series. This value was determined by using Guerreor's method and then manually checking values around it to verify its the best value for $\lambda$. 

### Differencing

To objectively determine if the dataset needs differencing we will use unitroot_kpss & unitroot_nsdiffs. 
```{r KPSS}
myseries %>%
    features(Turnover, unitroot_kpss) %>%
    kable(caption = "Kwiatkowski-Phillips-Schmidt-Shin Test")

myseries %>%
    features(difference(Turnover, 12), unitroot_kpss) %>%
    kable(caption = "Kwiatkowski-Phillips-Schmidt-Shin Test")
```

The KPSS testis performing a hypothesis test to verify that the data is stationary. However, the null hypothesis is rejected since the kpss_pvalue is less than 0.05, hence indicating that the data is not stationary and needs differencing to make it stationary.

After a first order differencing, the hypothesis test gives a p value of 0.10 which is greater than 0.05 which means the null hypothesis is not rejected. Thus making the first difference of the dataset, stationary. 

Based on Kwiatkowski-Phillips-Schmidt-Shin's Test, we have determined that the data needs differencing to make it stationary, while we tested with a first stage differencing we need to verify this using the unitroot_ndiffs which reveals the number of differencing that is needed. 

```{r Num of Diffs}
myseries %>%
    features(box_cox(Turnover, lambda), unitroot_ndiffs) %>%
    kable(caption = "Number of differences required for a stationary series")
```

According to unitroot_ndiffs the data only needs to perform a first stage differencing. 

```{r Num of Seasonal Diffs}
myseries %>%
    features(box_cox(Turnover, lambda), unitroot_nsdiffs) %>%
    kable(
        caption =
            "Number of seasonal differences required for a stationary series"
    )
```

According to unitroot_ndiffs the data only needs to perform a first stage seasonal differencing. 


# Modelling ARIMA and ETS models

```{r Creating Training Data, echo=FALSE}
training <- myseries %>%
    filter(Month < yearmonth("2017 Jan"))
```

### ETS Modelling

For the ETS models, the best method to short list possible candidate models is to use the AIC values that are outputed after models are trained. AIC, otherwise known as Akaike's Information Criterion, is defined as $AIC = T \log (\frac {SSE} {T}) + 2(k+2)$. Using it to find a model by minimising the AIC will result in a model that is good at forecasting. 

To find the model with the lowest AIC value, we need to check all the different models which can be done by `model(ETS(Turnover))` or manually testing all combinations. 

```{r TSCV Training Short Listing}
progressr::with_progress(
    trained_etsmodel <- training %>%
        model(
            # Additive
            ANN =  ETS(Turnover ~ error("A") + trend("N") + season("N")),
            ANA =  ETS(Turnover ~ error("A") + trend("N") + season("A")),
            ANM =  ETS(Turnover ~ error("A") + trend("N") + season("M")),
            AAN =  ETS(Turnover ~ error("A") + trend("A") + season("N")),
            AAA =  ETS(Turnover ~ error("A") + trend("A") + season("A")),
            AAM =  ETS(Turnover ~ error("A") + trend("A") + season("M")),
            AAdN = ETS(Turnover ~ error("A") + trend("Ad") + season("N")),
            AAdA = ETS(Turnover ~ error("A") + trend("Ad") + season("A")),
            AAdM = ETS(Turnover ~ error("A") + trend("Ad") + season("M")),

            # Multiplicative
            MNN =  ETS(Turnover ~ error("M") + trend("N") + season("N")),
            MNA =  ETS(Turnover ~ error("M") + trend("N") + season("A")),
            MNM =  ETS(Turnover ~ error("M") + trend("N") + season("M")),
            MAN =  ETS(Turnover ~ error("M") + trend("A") + season("N")),
            MAA =  ETS(Turnover ~ error("M") + trend("A") + season("A")),
            MAM =  ETS(Turnover ~ error("M") + trend("A") + season("M")), #*
            MAdN = ETS(Turnover ~ error("M") + trend("Ad") + season("N")),
            MAdA = ETS(Turnover ~ error("M") + trend("Ad") + season("A")),
            MAdM = ETS(Turnover ~ error("M") + trend("Ad") + season("M"))
        )
)

trained_etsmodel %>%
    glance() %>%
    select(.model, AIC, AICc) %>%
    arrange(AIC) %>%
    kable(caption = "ETS Models ranked based on AIC")
```

```{r Top six ets models, echo=FALSE}
top_ets <- trained_etsmodel %>%
    glance() %>%
    select(.model, AIC, AICc) %>%
    arrange(AIC) %>%
    head() %>%
    pull(.model)
```

By testing all combinations we see that the top 6 ETS models are: `r top_ets`. The list of ETS models can be further reduced by plotting a time series decomposition which shows the relationship between the seasonality and error components. 

```{r SLT Decomposition}
training %>%
    model(STL(
        # box_cox(Turnover, lambda) ~
        Turnover ~
            trend(window = 13) + season(window = "periodic"),
        robust = TRUE
    )) %>%
    components() %>%
    autoplot()
```

The decomposition shown in the graph indicates that the time series is multiplicative. Since the transformation done to the dataset was a box_cox transformation using a lambda value of `r lambda` which is closer to 0. And according to the box_cox transofrmation lambda value being zero is a log transformation. Hence the multiplicative decomposition.

So it can be deduced that the time series has a multiplicative relationship both the seasonality and error components which means the final short list of ETS models are: MNM, MAM, MAdM.

```{r TSCV Training, echo=FALSE}
progressr::with_progress(
    trained_etsmodel <- training %>%
        model(
            MNM = ETS(Turnover ~ error("M") + trend("N") + season("M")),
            MAM = ETS(Turnover ~ error("M") + trend("A") + season("M")), #*
            MAdM = ETS(Turnover ~ error("M") + trend("Ad") + season("M"))
        )
)
```

```{r Best ETS model identifier, echo=FALSE}
ets <- trained_etsmodel %>%
    select(mable_vars(trained_etsmodel)) %>%
    glance()
index <- ets$AIC %>% which.min()
```
After short listing, the best ETS model according to the AIC values is a `r ets[[index, ".model"]]` with a AIC value of `r ets[[index, "AICc"]]`

```{r Accuracy of ETS models}
bind_rows(
    trained_etsmodel %>%
        accuracy(),
    trained_etsmodel %>%
        forecast(h = "2 years") %>%
        accuracy(myseries)
) %>%
    arrange(MASE) %>%
    select(-State, -Industry, -ME, -MPE, -ACF1) %>%
    kable(caption = "Shortlist of the best ETS Models ranked on MASE")
```

After testing the models with the last 2 years of the dataset, the MNM model proves to be the best across RMSE, MAPE and MASE. The MAM and MAdM are also just as good with both tied for second place since their RMSE, MAPE and MASE values are marginally different. 

### ARIMA Modelling

From section Transformations and Differencings, we know that the dataset needs to transformed and differenced once to make the data stationary. So we can directly plot the ACF and PACF plots of the stationary dataset.

```{r ACF and PACF Plots, warning = FALSE}
training %>% gg_tsdisplay(
    difference(box_cox(Turnover, lambda), 12),
    plot_type = "partial", lag = 64
) + labs(title = "ACF and PACF plots of transformed and differenced Turnover")
```

The ACF plot shows a decaying sinusoidal pattern and by looking at the PACF the last  siginificant lag is at lag 3  which may suggest a non-seasonal AR(3).

There is exponential decay in the seasonal lags of the PACF which could indicate a seasonal MA(3) component. 

From the ACF and PACF plots we have a possible ARIMA model to start from ARIMA(box_cox(Turnover, lambda) ~ 1 + pdq(3, 0, 0) + PDQ(0, 1, 3))

To get better values we could make the p, P, q & Q values drift by 1 value and also checking with the addition of a constant.

```{r ARIMA Short Listing}
progressr::with_progress(
    trained_arimamodel <- training %>% model(
        ARIMA1300013 = ARIMA(box_cox(Turnover, lambda) ~ 1 + pdq(3, 0, 0) + PDQ(0, 1, 3)),
        ARIMA0300013 = ARIMA(box_cox(Turnover, lambda) ~ 0 + pdq(3, 0, 0) + PDQ(0, 1, 3)),
        ARIMA1200013 = ARIMA(box_cox(Turnover, lambda) ~ 1 + pdq(2, 0, 0) + PDQ(0, 1, 3)),
        ARIMA0200013 = ARIMA(box_cox(Turnover, lambda) ~ 0 + pdq(2, 0, 0) + PDQ(0, 1, 3)),
        ARIMA1201013 = ARIMA(box_cox(Turnover, lambda) ~ 1 + pdq(2, 0, 1) + PDQ(0, 1, 3)),
        ARIMA0201013 = ARIMA(box_cox(Turnover, lambda) ~ 0 + pdq(2, 0, 1) + PDQ(0, 1, 3)),
        ARIMA1300112 = ARIMA(box_cox(Turnover, lambda) ~ 1 + pdq(3, 0, 0) + PDQ(1, 1, 2)),
        ARIMA0300112 = ARIMA(box_cox(Turnover, lambda) ~ 0 + pdq(3, 0, 0) + PDQ(1, 1, 2)),
        ARIMA1200112 = ARIMA(box_cox(Turnover, lambda) ~ 1 + pdq(2, 0, 0) + PDQ(1, 1, 2)),
        ARIMA0200112 = ARIMA(box_cox(Turnover, lambda) ~ 0 + pdq(2, 0, 0) + PDQ(1, 1, 2))
    )
)

trained_arimamodel %>%
    glance() %>%
    select(.model, AIC, AICc) %>%
    arrange(AIC) %>%
    kable(caption = "ARIMA Models ranked based on AIC")
```

By checking slightly different variations of the initially obtained ARIMA model we can tell that ARIMA1300013 and ARIMA1300112 fit the data the best since their AIC values are very low and only 0.13 apart.

```{r Accuracy of ARIMA models}
bind_rows(
    trained_arimamodel %>%
        accuracy(),
    trained_arimamodel %>%
        forecast(h = "2 years") %>%
        accuracy(myseries)
) %>%
    arrange(MASE) %>%
    select(-State, -Industry, -ME, -MPE, -ACF1) %>%
    kable(caption = "Shortlist of the best ARIMA Models ranked on MASE")
```

After testing the models with the last 2 years of the dataset, the ARIMA1300013 and ARIMA1300112 models performed the did not perform the best during the testing phase. 

# Best Models for Forecasting

### Best ETS Model

```{r MAM Parameters}
MAM <- trained_etsmodel %>%
    select(MAM)

MAM %>%
    report()
```

The best ETS model was the MAM model since it had the lowest AIC value and during the testing set it also had a low MASE score which was not the lowest but relatively close to the best.

```{r MAM Diagnostic}
MAM %>%
    gg_tsresiduals() + labs(title = "ETS Innotvation Residuals Plot")

MAM %>%
    augment() %>%
    features(.innov, ljung_box, lag = 24, dof = 16) %>%
    kable(caption = "MAM Ljung Box Test")
```

From the innovation residuals plot and its corresponding ACF plot shows that there might still be some correlation that was missed. Which means that model is not perfect and will not provide the most accurate forecasts. With that being said, the the true value will still land with in its forecast interval.

```{r MAM Training Forecast}
MAM_fc <- MAM %>%
    forecast(h = "2 years")


MAM_fc %>%
    hilo(level = c(80, 95)) %>%
    select(-.model, -Turnover) %>%
    kable(caption = "ETS Point Forecast and Point Interval (80% & 95%)")

MAM_fc %>%
    autoplot() +
    autolayer(myseries %>% filter_index("Dec 2015" ~ .)) +
    labs(
        title = "MAM Point Forecast and Point Interval 2 years after 2016 Dec",
        y = "Turnover [million $AUD]",
        x = "Month [M]"
    )
```

### Best ARIMA Model

```{r ARIMA1300013 Parameters}
ARIMA1300013 <- trained_arimamodel %>%
    select(ARIMA1300013)

trained_arimamodel %>%
    accuracy() %>%
    arrange(MASE)

ARIMA1300013 %>%
    report()
```

The best ARIMA model was model ARIMA1300013 since it had the lowest AIC value and during the testing set it also had a low MASE score which was tied for second place with four other models and it was only 0.002 off first place.

```{r ARIMA1300013 Diagnostic}
ARIMA1300013 %>%
    gg_tsresiduals() + labs(title = "ARIMA Innotvation Residuals Plot")

ARIMA1300013 %>%
    augment() %>%
    features(.innov, ljung_box, lag = 12, dof = 6) %>%
    kable(caption = "ARIMA1300013 Ljung Box Test")
```

Similar to the ETS model, the innovation residuals plot and its corresponding ACF plot for the ARIMA model also shows that there might still be some correlation that was missed. Which means that model is not perfect and will not provide the most accurate forecasts. With that being said, the the true value will still land with in its forecast interval.

```{r ARIMA1300013 Training Forecast}
ARIMA1300013_fc <- ARIMA1300013 %>%
    forecast(h = "2 years")

ARIMA1300013_fc %>%
    hilo(level = c(80, 95)) %>%
    select(-.model, -Turnover) %>%
    kable(caption = "ARIMA Point Forecast and Point Interval (80% & 95%)")

ARIMA1300013_fc %>%
    autoplot() +
    autolayer(myseries %>% filter_index("Dec 2015" ~ .)) +
    labs(
        title = "ARIMA1300013 Point Forecast and Point Interval 2 years after 2016 Dec",
        y = "Turnover [million $AUD]",
        x = "Month [M]"
    )
```

# Comparison between ETS and ARIMA Models

```{r Model Comparison}
bind_rows(
    ARIMA1300013 %>% accuracy(),
    MAM %>% accuracy(),
    ARIMA1300013 %>% forecast(h = 24) %>% accuracy(myseries),
    MAM %>% forecast(h = 24) %>% accuracy(myseries)
) %>%
    select(-ME, -MPE, -ACF1) %>%
    arrange(MASE) %>%
    kable(caption = "Comparison between ETS and ARIMA Models")
```

After forecasting 2 years after the end of the training data, the accuracy of the ETS model triumphs the ARIMA model. The ETS model has a MASE value of 0.7415 while the ARIMA model had 2.1773. 

# Forecasting

```{r Full data training, echo=FALSE}
with_progress(
    fit <- myseries %>%
        model(
            ARIMA1300013 = ARIMA(
                box_cox(Turnover, lambda) ~ 1 + pdq(3, 0, 0) + PDQ(0, 1, 3)
                ),
            MAM = ETS(Turnover ~ error("M") + trend("A") + season("M"))
        )
)
```
```{r MAM forecasting}
fit %>%
    select(MAM) %>%
    forecast(h = "2 years") %>%
    hilo(level = 80) %>%
    select(-Turnover) %>%
    kable(
        caption = "MAM Point Forecast and Point Interval 2 years after 2018 Dec"
    )

fit %>%
    select(MAM) %>%
    forecast(h = "2 years") %>%
    autoplot() +
    autolayer(myseries %>% filter_index("Dec 2015" ~ .)) +
    labs(
        title = "MAM Point Forecast and Point Interval 2 years after 2018 Dec",
        y = "Turnover [million $AUD]",
        x = "Month [M]"
    )
```

```{r ARIMA1300013 forecasting}
fit %>%
    select(ARIMA1300013) %>%
    forecast(h = "2 years") %>%
    hilo(level = 80) %>%
    select(-Turnover) %>%
    kable(caption = "ARIMA1300013 Point Forecast and Point Interval 2 years after 2018 Dec")

fit %>%
    select(ARIMA1300013) %>%
    forecast(h = "2 years") %>%
    autoplot() +
    autolayer(myseries %>% filter_index("Dec 2015" ~ .)) +
    labs(
        title = "ARIMA1300013 Point Forecast and Point Interval 2 years after 2018 Dec",
        y = "Turnover [million $AUD]",
        x = "Month [M]"
    )
```

```{r Forecasting Plot, echo=FALSE}
fit %>%
    forecast(h = "2 years") %>%
    autoplot() +
    autolayer(myseries %>% filter_index("Dec 2015" ~ .)) +
    labs(
        title = "Point Forecast and Point Interval 2 years after 2018 Dec",
        subtitle = "MAM and ARIMA1300013",
        y = "Turnover [million $AUD]",
        x = "Month [M]"
    )
```

```{r ABS data, echo = FALSE, include = FALSE, warning=FALSE, error=FALSE}
true_data <- read_abs("8501.0", tables = 11)
true_data <- true_data %>%
    filter(series_id == "A3349433W") %>%
    mutate(Month = yearmonth(date)) %>%
    select(Month, value) %>%
    as_tsibble() %>%
    rename(Turnover = value)
```

```{r Comparing the forecasts with real values}
fit %>%
    forecast(h = "2 years") %>%
    accuracy(true_data)

fit %>%
    forecast(h = "2 years") %>%
    autoplot() +
    autolayer(true_data %>% filter_index("Dec 2015" ~ .)) +
    labs(
        title = "Point Forecast and Point Interval 2 years after 2018 Dec",
        subtitle = "ABSDATA",
        y = "Turnover [million $AUD]",
        x = "Month [M]"
    )
```

Checking the MASE of the forecasts and plots of the forecasts, we can tell that ARIMA1300013 did a decent job at predicting Turnover.
MAM model consistently underestiamted the mean Turnover but the true values were still within the prediction intervals. 

Overall the both models were quite limitting since both the ETS and ARIMA models did not have its residuals as the Ljung Box test which is use to verify that variations between the fitted values and the real data are only due to randomless and not due to unaccounted for correlation. The models were also considerably simpler since a more complex model could potentially better fit the training data and testing data but it could also be negatively effected with its ability to predict futre values. On the other hand a simpler model does not overfit the data which could allow it to better predict. So ultimately the best model would be somewhere in the middle. 